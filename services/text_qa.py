import os
from elasticsearch import Elasticsearch
import ollama
from sentence_transformers import CrossEncoder
import numpy as np
from typing import List, Tuple, Dict, Any

# K·∫øt n·ªëi Elasticsearch
ELASTIC_HOST = os.getenv('ELASTIC_HOST')
ELASTIC_PORT = os.getenv('ELASTIC_PORT')
es = Elasticsearch("http://{}:{}".format(ELASTIC_HOST, ELASTIC_PORT))


def vector_search(query: str, k: int = 3, index_name: str = 'text_embeddings') -> List[Tuple[str, float]]:
    """
    Perform vector search using Elasticsearch with cosine similarity.

    Args:
        query (str): The search query text
        k (int, optional): Number of results to return. Defaults to 3.
        index_name (str, optional): Elasticsearch index name. Defaults to 'text_embeddings'.

    Returns:
        List[Tuple[str, float]]: List of tuples containing (document_text, similarity_score)
    """
    # B∆∞·ªõc 1: L·∫•y embedding cho truy v·∫•n b·∫±ng Ollama (bge-m3)
    response = ollama.embed(model='bge-m3', input=query)
    query_embedding = response['embeddings'][0]  # L·∫•y m·∫£ng 1D

    # B∆∞·ªõc 2: X√¢y d·ª±ng truy v·∫•n script score cho Elasticsearch
    script_query = {
        "script_score": {
            "query": {
                "match_all": {}  # L·∫•y t·∫•t c·∫£ c√°c document
            },
            "script": {
                # cosineSimilarity tr·∫£ v·ªÅ gi√° tr·ªã trong [-1, 1], c·ªông th√™m 1 ƒë·ªÉ c√≥ gi√° tr·ªã d∆∞∆°ng
                "source": "cosineSimilarity(params.queryVector, 'embedding') + 1.0",
                "params": {
                    "queryVector": query_embedding
                }
            }
        }
    }

    # B∆∞·ªõc 3: G·ª≠i truy v·∫•n v√† l·∫•y top-k k·∫øt qu·∫£
    res = es.search(index=index_name, query=script_query, size=k)

    # B∆∞·ªõc 4: Tr√≠ch xu·∫•t text v√† score t·ª´ k·∫øt qu·∫£
    hits = res['hits']['hits']
    docs = []
    for hit in hits:
        source = hit['_source']
        score = hit['_score']
        docs.append((source["text"], score))

    return docs

# --- Sliding Window Extraction ---


# T·∫£i model cross-encoder ƒë·ªÉ ƒë√°nh gi√° relevance c·ªßa c√°c c·ª≠a s·ªï
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# def sliding_window_search(chunk, question, window_size=3, threshold=0.5):
#     """
#     T√°ch ƒëo·∫°n vƒÉn (chunk) th√†nh c√°c c·ª≠a s·ªï v·ªõi s·ªë c√¢u window_size,
#     sau ƒë√≥ ƒë√°nh gi√° relevance c·ªßa m·ªói c·ª≠a s·ªï v·ªõi c√¢u h·ªèi b·∫±ng cross-encoder.

#     Parameters:
#       - chunk: ƒëo·∫°n vƒÉn b·∫£n (string)
#       - question: c√¢u h·ªèi (string)
#       - window_size: s·ªë c√¢u trong m·ªói c·ª≠a s·ªï (m·∫∑c ƒë·ªãnh 3)
#       - threshold: ng∆∞·ª°ng ƒë·ªÉ ch·∫•p nh·∫≠n c·ª≠a s·ªï (m·∫∑c ƒë·ªãnh 0.5)

#     Returns:
#       - best_window: c·ª≠a s·ªï c√≥ ƒëi·ªÉm cao nh·∫•t (string) ho·∫∑c th√¥ng b√°o n·∫øu kh√¥ng ƒë·ªß relevance
#       - best_score: ƒëi·ªÉm c·ªßa c·ª≠a s·ªï ƒë√≥
#     """
#     # T√°ch chunk th√†nh c√°c c√¢u. L∆∞u √Ω: ƒëi·ªÅu ch·ªânh k√Ω t·ª± ph√¢n t√°ch n·∫øu c·∫ßn.
#     sentences = chunk.split('. ')
#     if len(sentences) < window_size:
#         windows = [" ".join(sentences)]
#     else:
#         windows = [" ".join(sentences[i:i+window_size]) for i in range(len(sentences) - window_size + 1)]

#     # T·∫°o c·∫∑p (question, window) cho cross-encoder
#     pairs = [(question, window) for window in windows]
#     scores = reranker.predict(pairs)

#     scores = np.array(scores)
#     best_index = int(scores.argmax())
#     best_score = float(scores[best_index])
#     best_window = windows[best_index]

#     if best_score >= threshold:
#         return best_window, best_score
#     else:
#         return "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p.", best_score

# # --- V√≠ d·ª• s·ª≠ d·ª•ng t√≠ch h·ª£p c·∫£ vector_search v√† Sliding Window ---
# if __name__ == "__main__":
#     # Gi·∫£ s·ª≠ truy v·∫•n c·ªßa ng∆∞·ªùi d√πng
#     query = "Chuy·ªÉn d·ªØ li·ªáu ra ng∆∞·ªõc ngo√†i"

#     # L·∫•y c√°c document c√≥ li√™n quan t·ª´ Elasticsearch
#     results = vector_search(query, k=3, index_name='text_embeddings')

#     print(f"üîç K·∫øt qu·∫£ t√¨m ki·∫øm cho c√¢u h·ªèi: {query}\n")
#     for i, (doc_text, score) in enumerate(results, start=1):
#         print(f"Document {i}: (Score: {score:.3f})")
#         print("Full text:")
#         print(doc_text)
#         # √Åp d·ª•ng chi·∫øn l∆∞·ª£c Sliding Window ƒë·ªÉ l·∫•y ra ph·∫ßn context ph√π h·ª£p nh·∫•t
#         best_window, window_score = sliding_window_search(doc_text, query, window_size=3, threshold=0.5)
#         print("\nExtracted best window:")
#         print(best_window)
#         print(f"Window relevance score: {window_score:.3f}")
#         print("-" * 60)


# ========================================================================================================#

query = "Vi·ªác chuy·ªÉn d·ªØ li·ªáu c√° nh√¢n ra n∆∞·ªõc ngo√†i"

results = vector_search(query, k=3, index_name='text_embeddings')

# In k·∫øt qu·∫£
for i, (doc_text, score) in enumerate(results, start=1):
    print(f"Document {i}:")
    print(f"Text: {doc_text}")
    print(f"Score: {score}")
    print()


def generate_answer(question: str, k: int = 3, index_name: str = 'text_embeddings') -> str:
    """
    Generate an answer for a given question using RAG approach.

    Args:
        question (str): The user's question
        k (int, optional): Number of documents to retrieve. Defaults to 3.
        index_name (str, optional): Elasticsearch index name. Defaults to 'text_embeddings'.

    Returns:
        str: Generated answer based on retrieved documents
    """
    # B∆∞·ªõc 1: L·∫•y c√°c t√†i li·ªáu li√™n quan
    retrieved_docs = vector_search(question, k, index_name)

    # B∆∞·ªõc 2: Chu·∫©n b·ªã prompt
    # Gh√©p t√†i li·ªáu th√†nh 1 chu·ªói context
    context_docs = "\n\n".join([f"- {doc[0]}" for doc in retrieved_docs])

    # Prompt v√≠ d·ª•:
    prompt = f"""
        cho c√¢u h·ªèi sau ƒë√¢y, H√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi th√¢n thi·ªán h∆°n - v√† l√†m r√µ t·ª´ t√†i li·ªáu n√†o trang bao nhi√™u
        Ch·ªâ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi, kh√¥ng n√≥i g√¨ h∆°n
        C√°c ƒëo·∫°n vƒÉn li√™n quan:
        {context_docs}

        C√¢u h·ªèi: {question}

        C√¢u tr·∫£ l·ªùi:
        """
    # print("Final prompt:\n", prompt)
    # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Ollama (ho·∫∑c m√¥ h√¨nh kh√°c)
    response = ollama.generate(
        model="llama3.2:1b",
        prompt=prompt
    )
    return response.get("response")


# # V√≠ d·ª• truy v·∫•n t·ª´ ng∆∞·ªùi d√πng:
# question = "Vi·ªác chuy·ªÉn d·ªØ li·ªáu c√° nh√¢n ra n∆∞·ªõc ngo√†i"

# # G·ªçi h√†m generate_answer
# answer = generate_answer(question, k=3, index_name='text_embeddings')

# # In k·∫øt qu·∫£ nh·∫≠n ƒë∆∞·ª£c
# print("C√¢u h·ªèi:", question)
# print("C√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh:", answer)
