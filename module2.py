import openai
from elasticsearch import Elasticsearch
import os
from dotenv import load_dotenv

load_dotenv()

# Thi·∫øt l·∫≠p OpenAI API Key (ƒê·∫∑t bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c hardcode ·ªü ƒë√¢y)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key")
openai.api_key = OPENAI_API_KEY

# K·∫øt n·ªëi Elasticsearch
es = Elasticsearch("http://localhost:9200")

print("Connected to Elasticsearch")

def get_embedding(text):
    """
    L·∫•y embedding t·ª´ m√¥ h√¨nh text-embedding-3-large c·ªßa OpenAI.
    """
    response = openai.embeddings.create(
        model="text-embedding-3-large",
        input=text
    )
    return response.data[0].embedding  # Tr·∫£ v·ªÅ vector embedding

def vector_search(query, k=3, index_name='text_embeddings_openai'):
    """
    T√¨m ki·∫øm vector b·∫±ng Elasticsearch, s·ª≠ d·ª•ng cosine similarity.
    """
    query_embedding = get_embedding(query)

    # X√¢y d·ª±ng truy v·∫•n script score
    script_query = {
        "script_score": {
            "query": {
                "match_all": {}  # L·∫•y t·∫•t c·∫£ docs, t√≠nh ƒëi·ªÉm theo vector similarity
            },
            "script": {
                "source": "cosineSimilarity(params.queryVector, 'embedding') + 1.0",
                "params": {
                    "queryVector": query_embedding
                }
            }
        }
    }

    # G·ª≠i truy v·∫•n t·ªõi Elasticsearch
    res = es.search(index=index_name, query=script_query, size=k)

    # Tr√≠ch xu·∫•t k·∫øt qu·∫£
    hits = res['hits']['hits']
    docs = [(hit['_source']['text'], hit['_score']) for hit in hits]
    
    return docs

def generate_answer(question, k=3, index_name='text_embeddings_openai'):
    """
    Sinh c√¢u tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu t·ª´ Elasticsearch v√† GPT-4o-mini.
    """
    retrieved_docs = vector_search(question, k, index_name)

    # Gh√©p c√°c t√†i li·ªáu th√†nh prompt cho GPT-4o-mini
    context_docs = "\n\n".join([f"- {doc[0]}" for doc in retrieved_docs])

    prompt = f"""
B·∫°n h√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ch·ªâ d·ª±a v√†o c√°c ƒëo·∫°n vƒÉn d∆∞·ªõi ƒë√¢y, kh√¥ng tr·∫£ l·ªùi nh·ªØng th√¥ng tin kh√¥ng li√™n quan:
{context_docs}

C√¢u h·ªèi: {question}

C√¢u tr·∫£ l·ªùi:
"""

    # G·ªçi GPT-4o-mini ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
    response = openai.chat.completions.create(
        model="gpt-4o-mini",  # S·ª≠ d·ª•ng GPT-4o-mini thay v√¨ LLaMA
        messages=[{"role": "system", "content": ("B·∫°n l√† tr·ª£ l√Ω chuy√™n gia trong vi·ªác tr√≠ch xu·∫•t th√¥ng tin t·ª´ m·ªôt vƒÉn b·∫£n cho tr∆∞·ªõc."
                                                 + "H√£y cung c·∫•p ch√≠nh x√°c t√™n file m√† b·∫°n ƒë√£ tr√≠ch xu·∫•t")},
                  {"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content  # Tr·∫£ v·ªÅ n·ªôi dung tr·∫£ l·ªùi

# V√≠ d·ª• truy v·∫•n
question = "D·ªØ li·ªáu c√° nh√¢n trong Ngh·ªã ƒê·ªãnh ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† g√¨?"
answer = generate_answer(question, k=3, index_name='text_embeddings_openai')

results = vector_search(question, k=3, index_name="text_embeddings_openai")

# # In k·∫øt qu·∫£
# print(f"\nüîç K·∫øt qu·∫£ t√¨m ki·∫øm cho: {question}")
# for i, (doc_text, score) in enumerate(results, start=1):
#     print(f"\nüìå Document {i}:")
#     print(f"üìù N·ªôi dung: {doc_text}")
#     print(f"‚≠ê Score: {score}")

# In k·∫øt qu·∫£
print("C√¢u h·ªèi:", question)
print("C√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh:", answer)